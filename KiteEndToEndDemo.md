---
layout: page
title: End-to-end Demo
---

This example demonstrates:

* Logging application events to Hadoop using Flume and Log4J
* Extracting session data from events using Crunch
* Analyzing session data in SQL using Impala or Hive

## Getting Started

This example assumes that you have Oracle VirtualBox or VMWare Fusion installed and have a running [Cloudera QuickStart VM][getvm] version 5.1 or later. See the configuration instructions on the [Kite Examples Setup](KiteExamplesSetup/) page.

In that VM, enter the following commands in a terminal window to clone the latest version of the example code and switch to the demo directory.

```bash
git clone https://github.com/kite-sdk/kite-examples.git
cd kite-examples
cd demo
```

[getvm]: http://www.cloudera.com/content/support/en/downloads/quickstart_vms.html

## Configuring the VM

Flume needs to be able to impersonate the owner of the dataset it is writing to. (This is similar to Unix `sudo`. See [Configuring Flume's Security Properties](http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDH5/latest/CDH5-Security-Guide/cdh5sg_flume_security_props.html#topic_4_2_1_unique_1) for further information.)

If you're using Cloudera Manager then this is already configured.  (The QuickStart VM ships with Cloudera Manager, but it is disable by default.) 

If you're not using Cloudera Manager, configure Flume as follows:

  * Add the XML property definition below to the `/etc/hadoop/conf/core-site.xml` file.

```
<property>
  <name>hadoop.proxyuser.flume.groups</name>
  <value>*</value>
</property>
<property>
  <name>hadoop.proxyuser.flume.hosts</name>
  <value>*</value>
</property>
```

  * Restart the NameNode with the shell command `sudo service hadoop-hdfs-namenode restart`.

### __Configure the Flume agent__

* Open the file `flume.properties` from the `kite-examples/demo` directory in a text editor.

* Verify that the value of `tier1.sinks.sink-1.auth.proxyUser` in the `flume.properties` file matches your login username. The default value is `cloudera`, which is correct for the QuickStart VM, but you'll likely need to change this when running the example from another system.

* If you're using Cloudera Manager, configure the Flume agent by following these steps:
  * Copy the contents of the file `flume.properties`.
  * Open Cloudera Manager.
  * On the Home page, click the Flume service.
  * Click the __Configuration__ tab.
  * Click the category __Agent Base Group__.
  * Paste the contents of the `flume.properties` file into the __Configuration File__ property text area.
  * Click __Save Changes__.

* If you're not using Cloudera Manager, configure the Flume agent by following these steps:
  * Open the file `/etc/default/flume-ng-agent`.
  * Add the new line `FLUME_AGENT_NAME=tier1` (this sets the default Flume agent name to match the one defined in the `flume.properties` file).
  * Save the file.
  * In a terminal window, run `sudo cp flume.properties /etc/flume-ng/conf/flume.conf` so that the Flume agent uses the new configuration file.

__NOTE:__ Don't start Flume immediately after updating the configuration. Flume requires the datasets to be created before it will start correctly.

## Building

To build the project, navigate to the `kite-examples/demo` directory in a terminal window. Enter the command:

```
mvn install
```

Maven creates the following artifacts:

* a JAR file containing the compiled Avro specific schemas `standard_event.avsc` and `session.avsc` (in `demo-core`)
* a WAR file for the webapp that logs application events (in `demo-logging-webapp`)
* a JAR file for running the Crunch job to transform events into sessions (in
`demo-crunch`)
* a WAR file for the webapp that displays reports generated by Impala (in
`demo-reports-webapp`)

## Running the Demo

### Create the datasets

First create the datasets: one named `events` to store the raw events,
and another named `sessions` for the derived sessions.

The raw events metadata is stored in HDFS so Flume can find the schema. The sessions dataset metadata is stored using HCatalog, so that you can query the dataset using Hive.

These commands create the events and sessions datasets when executed from a terminal window in the demo folder. The backslash (\) characters represent line breaks: these commands must be entered on a single line (single line versions follow these human-readable versions).

```
mvn kite:create-dataset \
  -Dkite.rootDirectory=/tmp/data \
  -Dkite.datasetName=events \
  -Dkite.avroSchemaFile=demo-core/src/main/avro/standard_event.avsc \
  -Dkite.hcatalog=false \
  -Dkite.partitionExpression='[year("timestamp", "year"), month("timestamp", "month"), day("timestamp", "day"), hour("timestamp", "hour"), minute("timestamp", "minute")]'

mvn kite:create-dataset \
  -Dkite.rootDirectory=/tmp/data \
  -Dkite.datasetName=sessions \
  -Dkite.avroSchemaFile=demo-core/src/main/avro/session.avsc
```

Here are single line versions of the same commands to make them easier to copy.

```
mvn kite:create-dataset -Dkite.rootDirectory=/tmp/data -Dkite.datasetName=events -Dkite.avroSchemaFile=demo-core/src/main/avro/standard_event.avsc -Dkite.hcatalog=false -Dkite.partitionExpression='[year("timestamp", "year"), month("timestamp", "month"), day("timestamp", "day"), hour("timestamp", "hour"), minute("timestamp", "minute")]'

mvn kite:create-dataset -Dkite.rootDirectory=/tmp/data -Dkite.datasetName=sessions -Dkite.avroSchemaFile=demo-core/src/main/avro/session.avsc`
```

A few comments about these commands. The schemas for the `events` and `sessions` datasets are loaded from local files.

The `-Dkite.partitionExpression` argument is used to specify that the data is partitioned by time fields, using JEXL to specify the field partitioners.

You can check that the data directories were created, using Hue (login as `cloudera/cloudera` if you are using the VM, or as your host login if you are running from the host machine): [`/tmp/data/default/events`](http://quickstart.cloudera:8888/filebrowser/#/tmp/data/default/events),
 [`/tmp/data/default/sessions`](http://quickstart.cloudera:8888/filebrowser/#/tmp/data/default/sessions).

### Start Flume

* If you're using Cloudera Manager:
    * Start (or restart) the Flume agent
* If you're not using Cloudera Manager:
    * Run `sudo /etc/init.d/flume-ng-agent restart` to restart the Flume agent with this new configuration

### Create events

Start an embedded Tomcat instance using Maven:

```
mvn tomcat7:run
```

In a web browser, open [http://quickstart.cloudera:8034/demo-logging-webapp/](http://quickstart.cloudera:8034/demo-logging-webapp/). This is a bare-bones web page you can use to send messages that are handled by the Flume agent. The agent writes the events you generate to the HDFS file sink.

Rather than creating lots of events manually, it's easier to simulate two users with a script. You can run the `simulate-activity.sh` script in a terminal window from the `demo` directory:

```
./bin/simulate-activity.sh 1 10 > /dev/null &
./bin/simulate-activity.sh 2 10 > /dev/null &
```

### Generate Derived Sessions

Wait about 30 seconds for Flume to flush the events to the
[filesystem](http://quickstart.cloudera:8888/filebrowser/#/tmp/data/default/events), then run the Crunch job to generate derived session data from the events:

```
cd demo-crunch
mvn kite:run-tool
```

The `kite:run-tool` Maven goal executes the `run` method of the `Tool`, in this case `CreateSessions`, which launches a Crunch job on the cluster.

The `Tool` class to run, as well as the cluster settings, are found from the configuration of the `kite-maven-plugin`.

When it's complete, you should see a file in [`/tmp/data/default/sessions`](http://quickstart.cloudera:8888/filebrowser/#/tmp/data/default/sessions).

Another way to invoke CreateSessions is to supply a view URI to process the events for a particular minute bucket:

```
mvn kite:run-tool -Dkite.args='view:hdfs:/tmp/data/default/events?year=2014&month=8&date=5&hour=17&minute=10'
```

### Run session analysis

The `sessions` dataset is now populated with data, but you need to tell Impala to refresh its metastore to make the new `sessions` table visible:

```
impala-shell -q 'invalidate metadata'
```

One way to explore the results is by using the `demo-reports-webapp` running at
[http://quickstart.cloudera:8034/demo-reports-webapp/](http://quickstart.cloudera:8034/demo-reports-webapp/). The web app uses JDBC to run Impala queries for a few pre-defined reports. (Note this only works with Impala 1.1 or later.)

Another way is to run ad hoc SQL queries using the Hue interfaces to
[Impala](http://quickstart.cloudera:8888/impala/) or [Hive](http://quickstart.cloudera:8888/beeswax/). Here are some queries you can try:

```
DESCRIBE sessions

SELECT * FROM sessions

SELECT AVG(duration) FROM sessions
```

### Delete the Datasets

When you are finished with the example datasets, or if you want to work through the example again, you can use these commands to delete the datasets. Open a terminal window and navigate to the `demo` directory to execute these commands.

```
mvn kite:delete-dataset -Dkite.rootDirectory=/tmp/data -Dkite.datasetName=events -Dkite.hcatalog=false
mvn kite:delete-dataset -Dkite.rootDirectory=/tmp/data -Dkite.datasetName=sessions
```