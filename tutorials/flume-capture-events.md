---
layout: page
title: Capturing Events with Flume
---

Once you have an [Events dataset][events], you can create a web application that captures session events.

This example shows how you can send log information via Flume to your Hadoop database using a JSP and custom servlet running on Tomcat.

[events]:{{site.baseurl}}/tutorials/create-events-dataset.html

## index.jsp

The default landing page for the web application is `index.jsp`. It defines a form with fields for an arbitrary User ID and a message. The __Send__ button submits the input values to the Tomcat server.

```JSP
<html>
  <head>
    <title>Kite Example</title>
  <head>
  <body>
    <h2>Kite Example</h2>
    <form name="input" action="send" method="get">
        User ID: <input type="text" name="user_id" value="1">
        Message: <input type="text" name="message" value="Hello!">
        <input type="submit" value="Send">
    </form>
  </body>
</html>
```

## LoggingServlet

When you submit a message from the JSP, the LoggingServlet receives and processes the request. The following is mostly standard servlet code, followed by some notes about application-specific snippets.

```Java
package org.kitesdk.examples.demo;
```

The servlet parses information from the request to create a StandardEvent object. However, you won't find any source code for `org.kitesdk.data.event.StandardEvent`. During the Maven build, the avro-maven-plugin runs before the compile phase. Any `.avsc` file in the `/main/avro` folder is defined as a Java class. The autogenerated classes have  the methods required to build corresponding Avro `SpecificRecord` objects of that type. `SpecificRecord` objects permit efficient access to object fields.

```Java

import org.kitesdk.data.event.StandardEvent;
import java.io.IOException;
import java.io.PrintWriter;
import javax.servlet.ServletException;
import javax.servlet.http.HttpServlet;
import javax.servlet.http.HttpServletRequest;
import javax.servlet.http.HttpServletResponse;

```

This example sends Log4j messages directly to the HDFS data sink via Flume.

```Java
import org.apache.log4j.Logger;

public class LoggingServlet extends HttpServlet {

  private final Logger logger = Logger.getLogger(LoggingServlet.class);

  @Override
  protected void doGet(HttpServletRequest request, HttpServletResponse
      response) throws ServletException, IOException {

    response.setContentType("text/html");
```    

Create a PrintWriter instance to write the response page.

```Java
  PrintWriter pw = response.getWriter();

    pw.println("<html>");
    pw.println("<head><title>Kite Example</title></head>");
    pw.println("<body>");
```

Get the user ID and message values from the servlet request.

```Java
    String userId = request.getParameter("user_id");
    String message = request.getParameter("message");
```

If there's no message, don't create a log entry.

```Java
    if (message == null) {
      pw.println("<p>No message specified.</p>");

```

Otherwise, print the message at the top of the page body.

```Java
    } else {
      pw.println("<p>Message: " + message + "</p>");
      
```

Create a new StandardEvent builder.

```Java
      StandardEvent event = StandardEvent.newBuilder()
```
The event initiator is a user on the server. The event is a web message. These can be set as string literals, because the event initiator and event name are always the same.

```Java
          .setEventInitiator("server_user")
          .setEventName("web:message")
```

Parse the arbitrary user ID, provided by the user, as a long integer.

```Java
          .setUserId(Long.parseLong(userId))

```

The application obtains the session ID and IP address from the request object, and creates a timestamp based on the local machine clock.

```Java
          .setSessionId(request.getSession(true).getId())
          .setIp(request.getRemoteAddr())
          .setTimestamp(System.currentTimeMillis())
```

Build the StandardEvent object, then send the object to the logger with the level _info_.

```Java
          .build();
      logger.info(event);
    }
    pw.println("<p><a href=\"/demo-logging-webapp\">Home</a></p>");
    pw.println("</body></html>");
  }
}
```

## Running the Web Application

Follow these steps to build the web application, start the Tomcat server, and then use the web application to generate events that are sent to the Hadoop dataset.

1. In a terminal window, navigate to `/kite-examples/demo`.
1. Type the command `mvn install`.
1. Restart the Flume agent
  If using Cloudera Manager: Start (or restart) the Flume agent.
  If not using Cloudera Manager: In the terminal window, enter  `sudo /etc/init.d/flume-ng-agent restart`.
1. In the terminal window, enter `mvn tomcat7:run`.
1. In a web browser, enter the URL [`http://quickstart.cloudera:8034/demo-logging-webapp/`][logging-app].
1. On the web form, enter any user ID and a message, and then click **Send** to create a web event. 

The Flume agent receives the events over inter-process communication (IPC), and the agent writes the events to the HDFS file sink. Each time you send a message, Log4j writes a new `INFO` line in the terminal window.

View the records in Hadoop using the Hue File Browser.

[http://quickstart.cloudera:8888/filebrowser/view/tmp/data/default/events](http://quickstart.cloudera:8888/filebrowser/view/tmp/data/default/events)

[logging-app]:http://quickstart.cloudera:8034/demo-logging-webapp/